{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow import keras\n",
    "import mlcrate as mlc\n",
    "import pickle as pkl\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Embedding, Dense, Flatten, Concatenate, Dot, Reshape, Add, Subtract\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.regularizers import l2\n",
    "from sklearn.base import clone\n",
    "from typing import Dict\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from tensorflow.keras.losses import Loss\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit, StratifiedKFold, KFold, GroupKFold\n",
    "from tqdm import tqdm\n",
    "from tensorflow.python.ops import math_ops\n",
    "%%time\n",
    "n_features = 300\n",
    "features = [f'f_{i}' for i in range(n_features)]\n",
    "feature_columns = ['investment_id', 'time_id'] + features\n",
    "train = pd.read_pickle('../input/ubiquant-market-prediction-half-precision-pickle/train.pkl')\n",
    "train.head()\n",
    "investment_id = train.pop(\"investment_id\")\n",
    "investment_id.head()\n",
    "_ = train.pop(\"time_id\")\n",
    "y = train.pop(\"target\")\n",
    "y.head()\n",
    "%%time\n",
    "investment_ids = list(investment_id.unique())\n",
    "investment_id_size = len(investment_ids) + 1\n",
    "investment_id_lookup_layer = layers.IntegerLookup(max_tokens=investment_id_size)\n",
    "with tf.device(\"cpu\"):\n",
    "  investment_id_lookup_layer.adapt(investment_id)\n",
    "def preprocess(X, y):\n",
    "    print(X)\n",
    "    print(y)\n",
    "    return X, y\n",
    "def make_dataset(feature, investment_id, y, batch_size=1024, mode=\"train\"):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature), y))\n",
    "    ds = ds.map(preprocess)\n",
    "    if mode == \"train\":\n",
    "        ds = ds.shuffle(256)\n",
    "    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return ds\n",
    "def get_model():\n",
    "    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n",
    "    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n",
    "\n",
    "    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n",
    "    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n",
    "    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n",
    "    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n",
    "    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n",
    "    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n",
    "\n",
    "    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n",
    "    feature_x = layers.Dense(256, activation='swish')(feature_x)\n",
    "    feature_x = layers.Dense(256, activation='swish')(feature_x)\n",
    "\n",
    "    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n",
    "    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    output = layers.Dense(1)(x)\n",
    "    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n",
    "    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n",
    "    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model2():\n",
    "    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n",
    "    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n",
    "\n",
    "    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n",
    "    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n",
    "    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n",
    "    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n",
    "    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n",
    "    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n",
    "    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n",
    "\n",
    "    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n",
    "    feature_x = layers.Dense(256, activation='swish')(feature_x)\n",
    "    feature_x = layers.Dense(256, activation='swish')(feature_x)\n",
    "    feature_x = layers.Dense(256, activation='swish')(feature_x)\n",
    "    feature_x = layers.Dropout(0.65)(feature_x)\n",
    "\n",
    "    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n",
    "    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "   # x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "  #  x = layers.Dropout(0.4)(x)\n",
    "    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dropout(0.75)(x)\n",
    "    output = layers.Dense(1)(x)\n",
    "    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n",
    "    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n",
    "    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_model3():\n",
    "    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n",
    "    features_inputs = tf.keras.Input((300, ), dtype=tf.float32)\n",
    "\n",
    "    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n",
    "    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n",
    "    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n",
    "    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n",
    "    investment_id_x = layers.Dropout(0.5)(investment_id_x)\n",
    "    investment_id_x = layers.Dense(32, activation='swish')(investment_id_x)\n",
    "    investment_id_x = layers.Dropout(0.5)(investment_id_x)\n",
    "    #investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n",
    "\n",
    "    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n",
    "    feature_x = layers.Dropout(0.5)(feature_x)\n",
    "    feature_x = layers.Dense(128, activation='swish')(feature_x)\n",
    "    feature_x = layers.Dropout(0.5)(feature_x)\n",
    "    feature_x = layers.Dense(64, activation='swish')(feature_x)\n",
    "\n",
    "    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(64, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    x = layers.Dense(16, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    output = layers.Dense(1)(x)\n",
    "    output = tf.keras.layers.BatchNormalization(axis=1)(output)\n",
    "    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n",
    "    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n",
    "    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n",
    "    return model\n",
    "\n",
    "def get_model5():\n",
    "    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n",
    "\n",
    "    ## feature ##\n",
    "    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n",
    "    feature_x = layers.Dropout(0.1)(feature_x)\n",
    "    ## convolution 1 ##\n",
    "    feature_x = layers.Reshape((-1,1))(feature_x)\n",
    "    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=1, padding='same')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU()(feature_x)\n",
    "    ## convolution 2 ##\n",
    "    feature_x = layers.Conv1D(filters=16, kernel_size=4, strides=4, padding='same')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU()(feature_x)\n",
    "    ## convolution 3 ##\n",
    "    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=1, padding='same')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU()(feature_x)\n",
    "    ## convolution 4 ##\n",
    "    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=4, padding='same')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU()(feature_x)\n",
    "    ## convolution 5 ##\n",
    "    feature_x = layers.Conv1D(filters=64, kernel_size=4, strides=2, padding='same')(feature_x)\n",
    "    feature_x = layers.BatchNormalization()(feature_x)\n",
    "    feature_x = layers.LeakyReLU()(feature_x)\n",
    "    ## flatten ##\n",
    "    feature_x = layers.Flatten()(feature_x)\n",
    "\n",
    "    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(feature_x)\n",
    "\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dropout(0.1)(x)\n",
    "    output = layers.Dense(1)(x)\n",
    "    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n",
    "    model = tf.keras.Model(inputs=[features_inputs], outputs=[output])\n",
    "    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n",
    "    return model\n",
    "models = []\n",
    "\n",
    "for i in range(5):\n",
    "    model = get_model()\n",
    "    model.load_weights(f'../input/dnn-base/model_{i}')\n",
    "    models.append(model)\n",
    "\n",
    "for i in range(10):\n",
    "    model = get_model2()\n",
    "    model.load_weights(f'../input/train-dnn-v2-10fold/model_{i}')\n",
    "    models.append(model)\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    model = get_model3()\n",
    "    model.load_weights(f'../input/dnnmodelnew/model_{i}')\n",
    "    models.append(model)\n",
    "\n",
    "\n",
    "models2 = []\n",
    "\n",
    "for i in range(5):\n",
    "    model = get_model5()\n",
    "    model.load_weights(f'../input/prediction-including-spatial-info-with-conv1d/model_{i}.tf')\n",
    "    models2.append(model)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "info = \">/dev/null 2>&1\"\n",
    "# info = \"\"\n",
    "!unzip ../input/ubiquant123/dnnlarge_norm -d ./ $info\n",
    "\n",
    "!python train.py --base_epoch 10 --folds 0 --model_name v1 --random_seed 27 $info\n",
    "!python predict.py $info\n",
    "\n",
    "for filedir in os.listdir(\"./\"):\n",
    "    if filedir != \"submission.csv\":\n",
    "        !rm -rf $filedir $info\n",
    "def get_model_dr04():\n",
    "    features_inputs = tf.keras.Input((300, ), dtype=tf.float32)\n",
    "\n",
    "    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n",
    "    feature_x = layers.Dropout(0.4)(feature_x)\n",
    "    feature_x = layers.Dense(128, activation='swish')(feature_x)\n",
    "    feature_x = layers.Dropout(0.4)(feature_x)\n",
    "    feature_x = layers.Dense(64, activation='swish')(feature_x)\n",
    "\n",
    "    x = layers.Concatenate(axis=1)([feature_x])\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    x = layers.Dense(64, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    x = layers.Dense(16, activation='swish', kernel_regularizer=\"l2\")(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    output = layers.Dense(1)(x)\n",
    "    output = tf.keras.layers.BatchNormalization(axis=1)(output)\n",
    "    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n",
    "    model = tf.keras.Model(inputs=[features_inputs], outputs=[output])\n",
    "    model.compile(optimizer=tf.optimizers.Adam(0.001),  loss = correlationLoss, metrics=[correlationMetric])\n",
    "    return model\n",
    "\n",
    "# dr=0.3\n",
    "\n",
    "# gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# for gpu in gpus:\n",
    "#     print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)\n",
    "\n",
    "# n_features = 300\n",
    "# features = [f'f_{i}' for i in range(n_features)]\n",
    "\n",
    "def preprocess(X, y):\n",
    "    return X, y\n",
    "def make_dataset(feature, y, batch_size=1024, mode=\"train\"):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((feature, y))\n",
    "    ds = ds.map(preprocess)\n",
    "    if mode == \"train\":\n",
    "        ds = ds.shuffle(512)\n",
    "#     ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "def correlationMetric(x, y, axis=-2):\n",
    "  \"\"\"Metric returning the Pearson correlation coefficient of two tensors over some axis, default -2.\"\"\"\n",
    "  x = tf.convert_to_tensor(x)\n",
    "  y = math_ops.cast(y, x.dtype)\n",
    "  n = tf.cast(tf.shape(x)[axis], x.dtype)\n",
    "  xsum = tf.reduce_sum(x, axis=axis)\n",
    "  ysum = tf.reduce_sum(y, axis=axis)\n",
    "  xmean = xsum / n\n",
    "  ymean = ysum / n\n",
    "  xvar = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n",
    "  yvar = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n",
    "  cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n",
    "  corr = cov / tf.sqrt(xvar * yvar)\n",
    "  return tf.constant(1.0, dtype=x.dtype) - corr\n",
    "\n",
    "\n",
    "def correlationLoss(x,y, axis=-2):\n",
    "  \"\"\"Loss function that maximizes the pearson correlation coefficient between the predicted values and the labels,\n",
    "  while trying to have the same mean and variance\"\"\"\n",
    "  x = tf.convert_to_tensor(x)\n",
    "  y = math_ops.cast(y, x.dtype)\n",
    "  n = tf.cast(tf.shape(x)[axis], x.dtype)\n",
    "  xsum = tf.reduce_sum(x, axis=axis)\n",
    "  ysum = tf.reduce_sum(y, axis=axis)\n",
    "  xmean = xsum / n\n",
    "  ymean = ysum / n\n",
    "  xsqsum = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n",
    "  ysqsum = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n",
    "  cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n",
    "  corr = cov / tf.sqrt(xsqsum * ysqsum)\n",
    "  return tf.convert_to_tensor( K.mean(tf.constant(1.0, dtype=x.dtype) - corr ) , dtype=tf.float32 )\n",
    "def correlationMetric_01mse(x, y, axis=-2):\n",
    "  \"\"\"Metric returning the Pearson correlation coefficient of two tensors over some axis, default -2.\"\"\"\n",
    "  x = tf.convert_to_tensor(x)\n",
    "  y = math_ops.cast(y, x.dtype)\n",
    "  n = tf.cast(tf.shape(x)[axis], x.dtype)\n",
    "  xsum = tf.reduce_sum(x, axis=axis)\n",
    "  ysum = tf.reduce_sum(y, axis=axis)\n",
    "  xmean = xsum / n\n",
    "  ymean = ysum / n\n",
    "  xvar = tf.reduce_sum( tf.math.squared_difference(x, xmean), axis=axis)\n",
    "  yvar = tf.reduce_sum( tf.math.squared_difference(y, ymean), axis=axis)\n",
    "  cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n",
    "  corr = cov / tf.sqrt(xvar * yvar)\n",
    "  return tf.constant(1.0, dtype=x.dtype) - corr\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "# list(GroupKFold(5).split(train , groups = train.index))[0]\n",
    "def pearson_coef(data):\n",
    "    return data.corr()['target']['preds']\n",
    "\n",
    "def evaluate_metric(valid_df):\n",
    "    return np.mean(valid_df[['time_id_', 'target', 'preds']].groupby('time_id').apply(pearson_coef))\n",
    "\n",
    "\n",
    "models3 = []\n",
    "\n",
    "for index in range(10):\n",
    "    model = get_model_dr04()\n",
    "    model.load_weights(f\"../input/model10mse/model_{index}\")\n",
    "    models3.append(model)\n",
    "\n",
    "is_submitss = False\n",
    "def preprocess_test(investment_id, feature):\n",
    "    return (investment_id, feature), 0\n",
    "\n",
    "def preprocess_test_s(feature):\n",
    "    return (feature), 0\n",
    "\n",
    "def make_test_dataset(feature, investment_id, batch_size=1024):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature)))\n",
    "    ds = ds.map(preprocess_test)\n",
    "    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "def make_test_dataset2(feature, batch_size=1024):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(((feature)))\n",
    "    ds = ds.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "def inference(models, ds):\n",
    "    y_preds = []\n",
    "    for model in models:\n",
    "        y_pred = model.predict(ds)\n",
    "        y_preds.append(y_pred)\n",
    "    return np.mean(y_preds, axis=0)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=1)\n",
    "\n",
    "\n",
    "def make_test_dataset3(feature, batch_size=1024):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((feature))\n",
    "    ds = ds.map(preprocess_test_s)\n",
    "    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "def infer(models, ds):\n",
    "    y_preds = []\n",
    "    for model in models:\n",
    "        y_pred = model.predict(ds)\n",
    "        y_preds.append((y_pred-y_pred.mean())/y_pred.std())\n",
    "\n",
    "    return np.mean(y_preds, axis=0)\n",
    "if is_submitss:\n",
    "    import ubiquant\n",
    "    env = ubiquant.make_env()\n",
    "    iter_test = env.iter_test()\n",
    "    for (test_df, sample_prediction_df) in iter_test:\n",
    "        ds = make_test_dataset(test_df[features], test_df[\"investment_id\"])\n",
    "        p1 = inference(models, ds)\n",
    "        ds2 = make_test_dataset2(test_df[features])\n",
    "        p2 = inference(models2, ds2)\n",
    "        ds3 = make_test_dataset3(test_df[features])\n",
    "        p3 = infer(models3, ds3)\n",
    "        sample_prediction_df['target'] = p1 * 0.29 + p2 * 0.64 + p3 * 0.07\n",
    "        env.predict(sample_prediction_df)\n",
    "        display(sample_prediction_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}